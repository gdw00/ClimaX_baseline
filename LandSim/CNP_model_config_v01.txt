# CNP Model default configuration
# Mirrors the current defaults in get_cnp_combined_config().
# Format: key = value. Lists can be comma-separated or Python lists.
# Section headers (in brackets) are optional and ignored by the parser.
# CNP Model configuration approximating the 3.8 M-parameter run (2025-10-07)

[ENCODERS]
# Time series token configuration
# Option 1: Specify number of tokens (recommended, more intuitive)
num_time_tokens = 12
# Option 2: Specify patch size in months (60 = 5 years per token)
# patch_size = 60  # Uncomment to use patch_size instead of num_time_tokens

# Surface/static encoder
static_fc_size = 256
# => Surface embedding: 128 (due to final // 2 in the model)

# PFT parameters encoder (use CNN by default for CNP model)
pft_param_cnn_channels = [32, 64, 128]
pft_param_cnn_kernel_size = 3
pft_param_cnn_padding = 1
use_cnn_for_pft_param = true
pft_param_size = 44
num_pfts = 17
# => PFT-parameter embedding (CNN + GAP): 64

# Water encoder (disabled by default in training; keep head size at 0)
water_fc_size = 0

# Scalar encoder
scalar_fc_size = 64
# => Scalar embedding: 32 (two-layer MLP projects to 32)

# 1D PFT encoder
pft_1d_fc_size = 256
# => PFT state embedding: 256

[SOIL2D_CNN]
# Soil 2D encoders (both 1D and 2D branches use these channel sizes)
conv_channels = [32, 64, 128]
conv_kernel_size = 3
conv_padding = 1
# => Soil 2D embedding: 128 (Flatten -> 128 -> 128 head)

[TRANSFORMER]
# Feature fusion transformer
num_tokens = 7
token_dim = 128
embed_dim = 128
transformer_layers = 8
transformer_heads = 16

# Global dropout probability
# Set to 0.0 for strict determinism in experiments if needed
dropout_p = 0.1

[OUTPUTS]
# Output geometry (do not set scalar/vector/matrix sizes here to avoid clashes with variable lists)
vector_length = 16
matrix_rows = 1
matrix_cols = 10

# Usage:
#   python train_cnp_model.py --model-config CNP_model_config.txt
#   python scripts/run_inference_all.py --model-config CNP_model_config.txt
